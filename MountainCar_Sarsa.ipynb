{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA: MountainCar problem\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This notebook solves the problem of the mountain car presented in (https://gym.openai.com/envs/MountainCar-v0/) using **SARSA** reinforcement learning algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "import gym\n",
    "import  numpy as np\n",
    "from gym import wrappers\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import Video\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting gym environment\n",
    "def get_env(env_name):\n",
    "    return gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the video saved at ./video/ location\n",
    "def show_video():\n",
    "    video_list = glob.glob('video/*.mp4')\n",
    "    if len(video_list) > 0:\n",
    "        ipythondisplay.display(HTML(\"\"\"\n",
    "                                    <video alt=\"test\" controls>\n",
    "                                        <source src=\"{}\" type=\"video/mp4\">\n",
    "                                    </video>\n",
    "                                \"\"\".format(video_list[0])))\n",
    "    else:\n",
    "        print('No videos found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup environment\n",
    "env = get_env('MountainCar-v0')\n",
    "env = wrappers.Monitor(env, 'video/', force=True)\n",
    "observation = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can try to play one episode of the game, using random actions at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps: 200\n",
      "Total reward: -200.0\n"
     ]
    }
   ],
   "source": [
    "# one initial episode using random actions\n",
    "done = False\n",
    "cnt = 0\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    cnt += 1\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "print(\"number of steps: {}\".format(cnt))\n",
    "print('Total reward: {}'.format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                                    <video alt=\"test\" controls>\n",
       "                                        <source src=\"video\\openaigym.video.0.3712.video000000.mp4\" type=\"video/mp4\">\n",
       "                                    </video>\n",
       "                                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "NUM_FEATURES = 2\n",
    "MIN_POS = -1.2\n",
    "MAX_POS = 0.6\n",
    "MIN_VELOCITY = -0.07 \n",
    "MAX_VELOCITY = 0.07\n",
    "NUM_STATES = 100\n",
    "NUM_ACTIONS = 3\n",
    "NUM_BINS = 10\n",
    "EPSILON_DECAY = 0.9\n",
    "EPSILON = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting discrete representation of continuous features\n",
    "def get_discrete_bins(ranges, num_bins=10):\n",
    "    bins = []\n",
    "    for r in ranges:\n",
    "        bins.append(np.linspace(r[0], r[1], num_bins))\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming state features from continuos to discrete\n",
    "def get_discrete_state(state):\n",
    "    bins = get_discrete_bins([(MIN_POS, MAX_POS), (MIN_VELOCITY, MAX_VELOCITY)])\n",
    "    discrete_state = np.zeros(NUM_FEATURES)\n",
    "    for i, feature in enumerate(state):\n",
    "        linspace = bins[i]\n",
    "        curr_bin = 0\n",
    "        while feature > linspace[curr_bin]:\n",
    "            curr_bin += 1\n",
    "        discrete_state[i] = curr_bin\n",
    "    return discrete_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps states between state number and tuple representing state features\n",
    "def map_states():\n",
    "    state_to_num = {}\n",
    "    num_to_state = {}\n",
    "    curr_num = 0\n",
    "    for i in range(NUM_BINS):\n",
    "        for j in range(NUM_BINS):\n",
    "            state_to_num[(i, j)] = curr_num\n",
    "            num_to_state[curr_num] = (i, j)\n",
    "            curr_num += 1\n",
    "    return state_to_num, num_to_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_num, num_to_state = map_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Q(num_states, num_actions):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = init_Q(NUM_STATES, NUM_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA parameters\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon-greedy policy for choosing action\n",
    "def choose_action(Q, epsilon, state_num):\n",
    "    action_type = np.random.binomial(n=1, p=epsilon, size=1)\n",
    "    if action_type == 1:\n",
    "        # perform random action\n",
    "        random_action = np.random.uniform(0, NUM_ACTIONS, size=1)\n",
    "        random_action = np.trunc(random_action[0])\n",
    "        return int(random_action)\n",
    "    else:\n",
    "        # perform the best action\n",
    "        best_action = np.argmax(Q[state_num])\n",
    "        return int(best_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon decay over episodes\n",
    "def decrease_epsilon(epsilon, episode_num):\n",
    "    return epsilon * EPSILON_DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(env, Q, num_episodes, epsilon):\n",
    "    for episode_num in tqdm(range(num_episodes)):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        epsilon = decrease_epsilon(epsilon, episode_num)\n",
    "        while not done:\n",
    "            # discretize old state\n",
    "            old_state = get_discrete_state(observation)\n",
    "            # save old state\n",
    "            old_state = state_to_num[tuple(old_state)]\n",
    "            # chose action\n",
    "            action = choose_action(Q, epsilon, old_state)\n",
    "            # make step\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            # discretize new state\n",
    "            new_state = get_discrete_state(observation)\n",
    "            # save new state\n",
    "            new_state = state_to_num[tuple(new_state)]\n",
    "            # get new action from new state\n",
    "            new_action = choose_action(Q, epsilon=epsilon, state_num=new_state)\n",
    "            # update Q value\n",
    "            Q[old_state, action] += ALPHA * (reward + GAMMA * Q[new_state, new_action] - Q[old_state, action])\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env =  get_env('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|███████████████████████████████████████████████████████████████████████████████▊                                                   | 6091/10000 [04:55<02:48, 23.20it/s]"
     ]
    }
   ],
   "source": [
    "train_sarsa(env, Q, num_episodes=10000, epsilon=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_optimal(env, Q):\n",
    "    # setup recording\n",
    "    env = wrappers.Monitor(env, 'video/', force=True)\n",
    "    observation = env.reset()\n",
    "    env.render()\n",
    "    # play one episode\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        discrete_observation = get_discrete_state(observation)\n",
    "        state_num = state_to_num[tuple(discrete_observation)]\n",
    "        action = np.argmax(Q[state_num])\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print('Total reward: {}'.format(total_reward))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env =  get_env('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_optimal(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
